{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Атрибут traget хранит номера категорий для текстов из обучающей выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доступ к самим текстам через атрибут data. Выведем текст и категорию случайного примера из обучающего датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic = rec.motorcycles\n",
      "\n",
      "hey... I'm pretty new to the wonderful world of motorcycles... I just\n",
      "bought\n",
      "a used 81 Kaw KZ650 CSR from a friend.... I was just wondering what kind of\n",
      "\n",
      "saddle bags I could get for it (since I know nothing about them)  are there\n",
      "bags for the gas tank?  how much would some cost, and how much do they\n",
      "hold?\n",
      "thanks for your advice!!!  I may be new to riding, but I love it\n",
      "already!!!!\n",
      ":)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 854\n",
    "print('Topic = {0}\\n'.format(newsgroups_train.target_names[newsgroups_train.target[n]]))\n",
    "print(newsgroups_train.data[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset({'or', 'even', 'onto', 'several', 'some', 'throughout', 'same', 'thereby', 'yourself', 'them', 'whole', 'how', 'would', 'mill', 'cannot', 'not', 'fifteen', 'hereby', 'nowhere', 'before', 'amount', 'what', 'an', 'might', 'they', 'that', 'co', 'yet', 'anywhere', 'for', 'ever', 'sy...'has', 'other', 'rather', 'seeming', 'seem', 'done', 'twenty', 'seemed', 'most', 'take', 'herself'}),\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names_len = len(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rand(weights): # generate random weighted sample \n",
    "    \n",
    "    weights_normed = np.sort(weights) / np.sum(weights)\n",
    "    weights_bounded = np.cumsum(weights_normed)\n",
    "\n",
    "    rand = np.random.rand() # random sample from a uniform distribution over [0, 1)\n",
    "    for i in range(len(weights)):\n",
    "        if(rand < weights_bounded[i]):\n",
    "            rand = i\n",
    "            break\n",
    "    return np.argsort(weights)[rand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([1,1,1,1,1,1,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(weighted_rand(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_topic = np.zeros(len(vectorizer.vocabulary_), dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vectorizer.vocabulary_)):       \n",
    "    word_to_topic[i] = weighted_rand(np.full(target_names_len, 1/target_names_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topic = np.zeros(len(newsgroups_train.target_names))           # Счетчик n_k\n",
    "num_topic_word = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))   # Счетчик n_k,w\n",
    "num_text_topic = np.zeros((len(newsgroups_train.data), len(newsgroups_train.target_names)))    # Счетчик n_d,k\n",
    "alpha = np.zeros(len(newsgroups_train.target_names))               # Распределение тем по текстам\n",
    "beta = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))  # Распределение тем по словам\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(newsgroups_train.data)):\n",
    "    alpha[newsgroups_train.target[i]] = alpha[newsgroups_train.target[i]] + 1\n",
    "    text = newsgroups_train.data[i]\n",
    "    beta[newsgroups_train.target[i]] = beta[newsgroups_train.target[i]] + vectorizer.transform([text])\n",
    "    x = np.resize(vectorizer.transform([text]).toarray(), len(vectorizer.vocabulary_))\n",
    "    b = np.argwhere(x)\n",
    "    c = word_to_topic[b]\n",
    "    for j in range(len(num_topic)):\n",
    "        num_text_topic[i, j] = len(c[(c == j)])\n",
    "        num_topic[j] = num_topic[j] + len(c[(c == j)])\n",
    "    text_transformed = vectorizer.inverse_transform(vectorizer.transform([text]))[0]\n",
    "    for j in range(len(text_transformed)):\n",
    "        word = vectorizer.vocabulary_.get(text_transformed[j])\n",
    "        num_topic_word[word_to_topic[word], word] = num_topic_word[word_to_topic[word], word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "      # По-хорошему, для хорошего результата, надо сделать несколько раз, но компьютер просто не тянул  \n",
    "for i in range(len(newsgroups_train.data)):\n",
    "    text = newsgroups_train.data[i]\n",
    "    text_transformed = vectorizer.inverse_transform(vectorizer.transform([text]))[0]\n",
    "    for j in range(len(text_transformed)):            \n",
    "        word = vectorizer.vocabulary_.get(text_transformed[j])    \n",
    "        topic = word_to_topic[word]\n",
    "        num_text_topic[i, topic] = num_text_topic[i, topic] - 1\n",
    "        num_topic[topic] = num_topic[topic] - 1\n",
    "        num_topic_word[topic, word] = num_topic_word[topic, word] - 1\n",
    "\n",
    "        p = np.zeros(len(num_topic))\n",
    "        for k in range(len(num_topic)):\n",
    "            p[k] = (num_text_topic[i, k] + alpha[k]) * (num_topic_word[k, word] + beta[k, word]) / (num_topic[k] + np.sum(beta[k]))\n",
    "        topic = weighted_rand(np.abs(p))\n",
    "        word_to_topic[word] = topic\n",
    "        num_text_topic[i, topic] = num_text_topic[i, topic] + 1\n",
    "        num_topic[topic] = num_topic[topic] + 1\n",
    "        num_topic_word[topic, word] = num_topic_word[topic, word] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выпишем топ-10 слов по каждому тегу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Top 10 words in the Topic = 1\n",
      "\n",
      "like know number run 12 original simple wanted april lines \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 2\n",
      "\n",
      "maybe unless home hardware friend bible personal thinking related drivers \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 3\n",
      "\n",
      "think want public version white press useful 26 prove member \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 4\n",
      "\n",
      "people day drive nice 24 ok win near parts gave \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 5\n",
      "\n",
      "using power place list computer making 18 future happen willing \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 6\n",
      "\n",
      "god quite example times given article money later similar source \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 7\n",
      "\n",
      "way new right better case 10 having line cause gets \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 8\n",
      "\n",
      "did true second heard software left law came 11 message \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 9\n",
      "\n",
      "ll doesn probably possible called send change hand mind check \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 10\n",
      "\n",
      "going long kind makes says word speed sorry access went \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 11\n",
      "\n",
      "need got question actually interested game short business face ways \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 12\n",
      "\n",
      "good life large saying net live standard open usually human \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 13\n",
      "\n",
      "things won data days key buy issue team posting provide \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 14\n",
      "\n",
      "make post thought goes play claim sell ones took dos \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 15\n",
      "\n",
      "believe end program free 50 000 longer sounds office accept \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 16\n",
      "\n",
      "course isn general car talk assume jesus position company according \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 17\n",
      "\n",
      "real windows 15 1993 started david knows board guy save \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 18\n",
      "\n",
      "really work years mean order small 100 anybody phone american \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 19\n",
      "\n",
      "help person control told known interesting currently total bought looked \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 20\n",
      "\n",
      "say problem different world based exactly 17 needs children effect \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inverse_dict = {v:k  for k,v in vectorizer.vocabulary_.items()}\n",
    "for i in range(len(newsgroups_train.target_names)):\n",
    "    #print('    Top 10 words in the Topic = {0}'.format(newsgroups_train.target_names[i]))\n",
    "    print('    Top 10 words in the Topic = '+str(i+1))\n",
    "    print()\n",
    "    x = np.argsort(num_topic_word[i]) [word_to_topic[np.argsort(num_topic_word[i])] == i] [:-11:-1]\n",
    "    for j in range(len(x)):\n",
    "        print(inverse_dict.get(x[j]), end = ' ')\n",
    "    print()\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
